import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("global_electricity_production_data.csv")

# Print column names
print("Columns in dataset:", df.columns)

# Drop unwanted columns (Only if they exist)
unwanted_columns = ['parameter', 'date']  # Remove 'date' since it's not numeric
df.drop(columns=[col for col in unwanted_columns if col in df.columns], inplace=True)

# Convert categorical columns to numeric using LabelEncoder
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
   le = LabelEncoder()
   df[col] = le.fit_transform(df[col])
   label_encoders[col] = le

# Print updated dataset info
print("Updated dataset columns:", df.columns)

# Fill missing values (Only for numeric columns)
df.fillna(df.select_dtypes(include=['number']).mean(), inplace=True)

# Define features and target
target_column = "country_name"  # Change this

if target_column in df.columns:
   X = df.drop(columns=[target_column])
   y = df[target_column]
else:
   raise ValueError(f"Target column '{target_column}' not found in dataset!")

# Feature selection
selector = SelectKBest(score_func=f_classif, k=min(5, X.shape[1]))  # Avoid errors if <5 features
X_selected = selector.fit_transform(X, y)

# Get selected feature names
selected_features = X.columns[selector.get_support()]
print("Selected Features:", selected_features)

# Normalize data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_selected)

# Standardize before PCA
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X_selected)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 principal components
X_pca = pca.fit_transform(X_standardized)

# Explained variance
print("Explained Variance Ratio:", pca.explained_variance_ratio_)

# Visualize PCA
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis', edgecolors='k')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Result Visualization")
plt.colorbar(label="Target Classes")
plt.show()
